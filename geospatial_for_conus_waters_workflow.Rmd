---
title: "geospatial_for_conus_waters_workflow"
author: "Kathryn Willi"
date: "2023-02-20"
output: html_document
---

```{r setup, include=TRUE, echo = T, warning = F, comment = F, message = F}
library(sf)
library(tidyverse)
library(terra)
library(nhdplusTools)
library(mapview)
library(dataRetrieval)
library(lubridate)
library(prism)
library(ggspatial)

# this gives you an error, but it can be ignored:
try(plyr::ldply(list.files(path="src/",
                       pattern="*.R",
                       full.names=TRUE),
            source))
# Rmarkdown options
knitr::opts_chunk$set(echo = T, warning = F, comment = F, message = F)

# mapview options
mapviewOptions(basemaps.color.shuffle=FALSE,basemaps='OpenTopoMap')
```

### Setting up your site data set.

For this code to run properly, your site data must be configured as follows:

1)  Each site is identified with a unique site name. In the data set, this column must be called `site`.
2)  Each site has coordinates, with column names `longitude` and `latitude`. Knowledge of coordinate projection required.
3)  Site data table is a CSV, and stored in the `data/` folder. 

I have included an example data set called `placeholder.csv`, along with all of the additional data sets necessary for the code to run, stored in the `data` folder. 

#### Downloading necessary data sets

Currently, this workflow requires downloading several data sets locally for much speedier run times. This includes: PRISM climate & aridity rasters, NHD flow direction data, and CONUS-wide NHD catchments. All data sets are found in the shared `data` folder.

### Site data assumptions.

This analysis is only appropriate for locations along adequately-sized streams. Some streams are too small to be captured by NHDPlusV2; it is also common for coordinates to fall in the wrong catchment (especially for big rivers). For that reason, review each site and make sure that the NHD feature attributes were appropriately captured.

# National Hydrodraphy Dataset (NHD) data extraction

Identify each sample's NHD COMID. This COMID will allow site linkages with all datasets in this workflow.

```{r}
sf_use_s2(FALSE)

sites <- read_csv("data/placeholder.csv") %>%
  st_as_sf(coords = c("longitude","latitude"), crs = 4269) # 4269 = NAD83 CRS

if(st_crs(sites) != st_crs(4269)){
  sites <- sites %>% st_transform(., crs = 4269)
}

mapview(sites)
```

Pull all meta data associated with each site's COMID.

```{r}
sites <- getNHD(df = sites)
```

Make NHD-based watershed shapefiles for all CONUS sites. To make this step MUCH faster, it is best to have a locally downloaded version on the National NHD catchment shapefile stored on your local system. I have already included this shapefile in the `data` folder. 

```{r}
site_watersheds <- getWatersheds(df = sites, make_pretty = TRUE) %>%
  inner_join(., select(st_drop_geometry(sites), site, comid), by = "comid")
```

Ensure that each site is accurately represented by the NHD (some locations may be located on streams that are too small to be captured by the NHD, others may have coordinates that are slightly off, potentially placing them in the wrong catchment). Here, we create simple maps of each watershed, to determine if the delineated watershed/NHD attributes are appropriate. This can take a while depending on how many sites you are doing this for. Maps are automatically stored in the `data/maps/` folder. It is highly recommended to review each map, particularly for known locations along BIG rivers and TINY streams.

```{r}
map2(sites$site, sites$comid, getMaps) 
```

Interactive map showing all sites and their delineated watersheds:

```{r}
mapview(site_watersheds, col.regions = "#56B4E9", alpha.regions = 0.2, lwd = 3, layer.name = "Watershed") +
  
mapview(sites, cex = 5, col.regions = "black", layer.name = "Points") + 
  
mapview(st_read('data/site_flowlines.gpkg', quiet = T), lwd = 3, color = "red", layer.name = "Flowline")
```

# StreamCat data extractions

This `getStreamCat()` function is adapted from [Simon Topp's Lakecat extraction](https://github.com/SimonTopp/USLakeClarityTrendr/blob/master/1_nhd_join_and_munge.Rmd). StreamCat is *huge* (\~600 possible variables). And while EPA has since made an [API that interacts with StreamCat](https://www.epa.gov/national-aquatic-resource-surveys/streamcat-metrics-rest-api) (which would make this code 1 billion times faster), it wasn't public when this code was written. So! We made a function that:

1)  Downloads StreamCat categories of data (e.g. dam density, urbanization, etc.) for all regions of CONUS.
2)  Joins that data to our sites by their NHD comid.
3)  Then, hilariously, deletes (or not, depending on if you want to keep it) the large gigabytes of data we don't use and only keeps the data that matches our sites' NHD comids.

To download the data that you want, be sure to update the `epa_categories` vector argument in `getStreamCat()` with the names of the categorized data sets you are interested in downloading. A list of those data sets can be found [here](https://gaftp.epa.gov/epadatacommons/ORD/NHDPlusLandscapeAttributes/StreamCat/HydroRegions/). Change `save = TRUE` to `save = FALSE` if you do not want to keep all CONUS-level StreamCat data that you downloaded.

```{r}
sites <- getStreamCat(sites = sites,
             # Choose EPA categories to download here. These example two categories took a few minutes to download. 
             epa_categories = c("ImperviousSurfaces", "Dams"),
             save = FALSE)
```

# Pulling additional geospatial data not in StreamCat

Lastly, we pull in additional data that is not found in StreamCat. This includes [PRISM climate data](<https://prism.oregonstate.edu/normals/>), [aridity data](<https://figshare.com/articles/dataset/Global_Aridity_Index_and_Potential_Evapotranspiration_ET0_Climate_Database_v2/7504448/6>), and [Omernik Ecoregion data](<https://www.epa.gov/eco-research/level-iii-and-iv-ecoregions-continental-united-states>) for each site's coordinates (i.e., at the site location, NOT aggregated across its watershed). We also calculate mean aridity and dominant Ecoregion across site watersheds. Our [net primary production](https://lpdaac.usgs.gov/products/mod17a3hgfv006/) layer is no longer available from NASA so this data set has been excluded from this data pull. I plan to update this workflow to include it again once it becomes available.

```{r}
# Extract the mean aridity index within each site's watershed as well as each site's location
sites <- getAridity(df = sites, sf = site_watersheds)

# Extract Omernik ecoregion for each site's location
sites <- getOmernikSite(df = sites)

# Extract dominant Omernik ecoregion within each site's watershed
sites <- getOmernikWs(df = sites, sf = site_watersheds)

# Extract PRSIM ppt, tmean, tmax, and tmin data for each site's location
sites <- getPRISM(df = sites)

# Link to original NPP data set:
# https://lpdaac.usgs.gov/products/mod17a3hgfv006/ "Terra MODIS Net Primary Production Yearly L4 Global 500 m SIN Grid products are currently unavailable due to unexpected errors in the input data. Please note that a newer version of MODIS land products is available and plans are being developed for the retirement of Version 6 MODIS data products. Users are advised to transition to the improved Version 6.1 products as soon as possible."
```

Flag sites whose watersheds extend outside CONUS. For these sites, watershed statistics may not be appropriate, since they only represent the area within CONUS. Here, I am saying that any watershed that is less than 97% within the US is "international".

```{r}
final_data <- sites %>% 
  st_drop_geometry() %>%
  select(site, starts_with('WSPctFull')) %>%
  select(site, international = 2) %>%
  mutate(international = ifelse(international <= 97, "Largely International", "Within CONUS")) %>%
  left_join(select(sites, -starts_with('WSPctFull')), ., by='site')
```

